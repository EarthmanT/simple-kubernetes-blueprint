tosca_definitions_version: cloudify_dsl_1_3

description: >
  This blueprint creates a Kubernetes Cluster.
  It includes a master and two or more nodes with auto-scaling and auto-healing of the nodes.
  It is based on the Kubernetes Portable Multi-Node Cluster guide in the Kubernetes documentation website.
  https://kubernetes.io/docs/getting-started-guides/docker-multinode/

imports:
  - http://www.getcloudify.org/spec/cloudify/4.0/types.yaml
  - http://getcloudify.org.s3.amazonaws.com/spec/aws-plugin/1.4.4/plugin.yaml
  - http://www.getcloudify.org/spec/fabric-plugin/1.3.1/plugin.yaml
  - http://www.getcloudify.org/spec/diamond-plugin/1.3.5/plugin.yaml
  - types/scale.yaml
  - imports/kubernetes-blueprint.yaml

inputs:

  key_name:
    default: kubernetes-blueprint-key

  private_key_path:
    default: ~/.ssh/kubernetes-blueprint-key.pem

  vpc_id:
    type: string

  vpc_cidr_block:
    type: string

  public_subnet_id:
    type: string

  public_subnet_cidr:
    type: string

  private_subnet_id:
    type: string

  private_subnet_cidr:
    type: string

  ec2_region_name:
    default: us-east-1

  ec2_region_endpoint:
    default: ec2.us-east-1.amazonaws.com

  availability_zone:
    default: us-east-1e

  ami:
    description: >
      Amazon Ubuntu 14.04 AMI

  instance_type:
    description: >
      Agent VM Instance Type

  agent_user:
    default: ubuntu

dsl_definitions:

    aws_config: &aws_config
      aws_access_key_id: { get_secret: aws_access_key_id }
      aws_secret_access_key: { get_secret: aws_secret_access_key }
      ec2_region_name: { get_input: ec2_region_name }
      ec2_region_endpoint: { get_input: ec2_region_endpoint }

node_templates:

  kubernetes_master_host:
    type: cloudify.aws.nodes.Instance
    properties:
      agent_config:
        install_method: remote
        port: 22
        user: { get_input: agent_user }
        key: { get_property: [ key, private_key_path ] }
        min_workers: 2
      aws_config: *aws_config
      image_id: { get_input: ami }
      instance_type: { get_input: instance_type }
      parameters:
        user_data: |
          #!/bin/bash
          sudo groupadd docker
          sudo gpasswd -a ubuntu docker
        placement: { get_property: [ public_subnet, availability_zone ] }
    relationships:
      - type: cloudify.aws.relationships.instance_connected_to_keypair
        target: key
      - type: cloudify.aws.relationships.instance_connected_to_subnet
        target: public_subnet
      - type: cloudify.aws.relationships.instance_connected_to_security_group
        target: ssh_group
      - type: cloudify.aws.relationships.instance_connected_to_security_group
        target: kubernetes_security_group
      - type: cloudify.aws.relationships.instance_connected_to_elastic_ip
        target: kubernetes_master_ip

  kubernetes_node_host:
    type: cloudify.aws.nodes.Instance
    properties:
      agent_config:
        install_method: remote
        port: 22
        user: { get_input: agent_user }
        key: { get_property: [ key, private_key_path ] }
        min_workers: 2
      aws_config: *aws_config
      image_id: { get_input: ami }
      instance_type: { get_input: instance_type }
      parameters:
        user_data: |
          #!/bin/bash
          sudo groupadd docker
          sudo gpasswd -a ubuntu docker
        placement: { get_property: [ private_subnet, availability_zone ] }
    relationships:
      - type: cloudify.aws.relationships.instance_connected_to_keypair
        target: key
      - type: cloudify.aws.relationships.instance_connected_to_subnet
        target: private_subnet
      - type: cloudify.aws.relationships.instance_connected_to_security_group
        target: ssh_group
      - type: cloudify.aws.relationships.instance_connected_to_security_group
        target: kubernetes_security_group
    interfaces:
      cloudify.interfaces.monitoring_agent:
        install:
          implementation: diamond.diamond_agent.tasks.install
          inputs:
            diamond_config:
              interval: 1
        start: diamond.diamond_agent.tasks.start
        stop: diamond.diamond_agent.tasks.stop
        uninstall: diamond.diamond_agent.tasks.uninstall
      cloudify.interfaces.monitoring:
        start:
          implementation: diamond.diamond_agent.tasks.add_collectors
          inputs:
            collectors_config:
              ProcessResourcesCollector:
                config:
                  enabled: true
                  unit: B
                  measure_collector_time: true
                  cpu_interval: 0.5
                  process:
                    hyperkube:
                      name: hyperkube

  kubernetes_security_group:
    type: cloudify.aws.nodes.SecurityGroup
    properties:
      aws_config: *aws_config
      description: Security group for Kubernetes Cluster
      rules:
        - ip_protocol: tcp
          from_port: 53
          to_port: 53
          cidr_ip: 0.0.0.0/0
        - ip_protocol: udp
          from_port: 53
          to_port: 53
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 80
          to_port: 80
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 443
          to_port: 443
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 2379
          to_port: 2379
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 4001
          to_port: 4001
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 6443
          to_port: 6443
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 8000
          to_port: 8000
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 8080
          to_port: 8080
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 9090
          to_port: 9090
          cidr_ip: 0.0.0.0/0
        - ip_protocol: tcp
          from_port: 10250
          to_port: 10250
          cidr_ip: 0.0.0.0/0
    relationships:
      - type: cloudify.aws.relationships.security_group_contained_in_vpc
        target: vpc

  ssh_group:
    type: cloudify.aws.nodes.SecurityGroup
    properties:
      aws_config: *aws_config
      description: Puppet Group
      rules:
        - ip_protocol: tcp
          from_port: 22
          to_port: 22
          cidr_ip: { get_input: vpc_cidr_block }
    relationships:
      - type: cloudify.aws.relationships.security_group_contained_in_vpc
        target: vpc

  public_subnet:
    type: cloudify.aws.nodes.Subnet
    properties:
      aws_config: *aws_config
      use_external_resource: true
      resource_id: { get_input: public_subnet_id }
      cidr_block: { get_input: public_subnet_cidr }
      availability_zone: { get_input: availability_zone }
    relationships:
      - type: cloudify.aws.relationships.subnet_contained_in_vpc
        target: vpc

  private_subnet:
    type: cloudify.aws.nodes.Subnet
    properties:
      aws_config: *aws_config
      use_external_resource: true
      resource_id: { get_input: private_subnet_id }
      cidr_block: { get_input: private_subnet_cidr }
      availability_zone: { get_input: availability_zone }
    relationships:
      - type: cloudify.aws.relationships.subnet_contained_in_vpc
        target: vpc

  vpc:
    type: cloudify.aws.nodes.VPC
    properties:
      aws_config: *aws_config
      use_external_resource: true
      resource_id: { get_input: vpc_id }
      cidr_block: { get_input: vpc_cidr_block }

  key:
    type: cloudify.aws.nodes.KeyPair
    properties:
      aws_config: *aws_config
      resource_id: { get_input: key_name }
      private_key_path: { get_input: private_key_path }

  kubernetes_master_ip:
    type: cloudify.aws.nodes.ElasticIP
    properties:
      aws_config: *aws_config
      domain: vpc

groups:

 scale_up_group:
   members: [kubernetes_node_host]
    # This defines a scale group whose members may be scaled up, incrementing by 1.
    # The scale worflow is called when the following criteria are met
    # The Hyperkube process total CPU will be more than 3 for a total of 10 seconds.
    # No more than 6 hosts will be allowed.
   policies:
     auto_scale_up:
       type: scale_policy_type
       properties:
         policy_operates_on_group: true
         scale_limit: 6
         scale_direction: '<'
         scale_threshold: 30
         #service_selector: .*kubernetes_node_host.*.cpu.total.user
         service_selector: .*kubernetes_node_host.*cpu.total.user
         cooldown_time: 60
       triggers:
         execute_scale_workflow:
           type: cloudify.policies.triggers.execute_workflow
           parameters:
             workflow: scale
             workflow_parameters:
               delta: 1
               scalable_entity_name: kubernetes_node
               scale_compute: true

 scale_down_group:
   # This defines a scale group whose members may be scaled down. Only one host will be removed per run.
   # The scale worflow is called when the following criteria are met
   # The Hyperkube process total CPU will be less than 1 for a total of 200 seconds.
   # No less than 2 hosts will be allowed.
   members: [kubernetes_node_host]
   policies:
     auto_scale_down:
       type: scale_policy_type
       properties:
         scale_limit: 2
         scale_direction: '>'
         scale_threshold: 25
         #service_selector: .*kubernetes_node_host.*.process.hyperkube.cpu.percent
         service_selector: .*kubernetes_node_host.*cpu.total.user
         cooldown_time: 60
         moving_window_size: 30
       triggers:
         execute_scale_workflow:
           type: cloudify.policies.triggers.execute_workflow
           parameters:
             workflow: scale
             workflow_parameters:
               delta: -1
               scalable_entity_name: kubernetes_node
               scale_compute: true

 heal_group:
   # This defines a group of hosts in members that may be healed.
   # The heal workflow is called when a the following policy criteria are met.
   # Either the hyperkube process on the host, or the total host CPU need fall silent.
   # The host and all software that it is supposed to have running on it will be healed.
   members: [kubernetes_node_host]
   policies:
     simple_autoheal_policy:
       type: cloudify.policies.types.host_failure
       properties:
         service:
           - .*kubernetes_node_host.*.cpu.total.system
           - .*kubernetes_node_host.*.process.hyperkube.cpu.percent
         interval_between_workflows: 60
       triggers:
         auto_heal_trigger:
           type: cloudify.policies.triggers.execute_workflow
           parameters:
             workflow: heal
             workflow_parameters:
               node_instance_id: { 'get_property': [ SELF, node_id ] }
               diagnose_value: { 'get_property': [ SELF, diagnose ] }

outputs:
  kubernetes_info:
    description: Kubernetes Dashboard URL
    value:
      url: {concat: ["http://",{ get_attribute: [ kubernetes_master_ip, aws_resource_id ]},":8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard" ] }
