tosca_definitions_version: cloudify_dsl_1_3

imports:
  - http://www.getcloudify.org/spec/cloudify/3.4/types.yaml
  - http://www.getcloudify.org/spec/fabric-plugin/1.3.1/plugin.yaml
  - http://www.getcloudify.org/spec/diamond-plugin/1.3.3/plugin.yaml
  - https://raw.githubusercontent.com/cloudify-incubator/cloudify-oraclebmc-plugin/master/plugin.yaml
  - types/scale.yaml

dsl_definitions:
  bmc_config: &bmc_config
    user: 
    fingerprint: 
    key_file: 
    tenancy: 
    region: 

  hyperkube_monitoring: &hyperkube_monitoring
    collectors_config:
      CPUCollector: {}
      MemoryCollector: {}
      LoadAverageCollector: {}
      DiskUsageCollector:
        config:
          devices: sd[a-z]+[0-9]*$
      NetworkCollector: {}
      ProcessResourcesCollector:
        config:
          enabled: true
          unit: B
          measure_collector_time: true
          interval: 1
          process:
            hyperkube:
              name: hyperkube

inputs:
  ssh_user:
    default: opc
  ssh_keyfile:
    default: ''
  master_key:
    default: ''
  worker_key:
    default: ''
  master_image:
    description: image (must be Oracle Linux)
  master_shape:
    description: flavor
  worker_image:
    description: image
  worker_shape:
    description: flavor
  availability_domain:
    description: availability domain

node_types:
  fabric_host:
    derived_from: cloudify.oraclebmc.nodes.Instance
    properties:
      ssh_keyfile:
        type: string
        default: { get_input: ssh_keyfile }

node_templates:

  master:
    type: cloudify.nodes.SoftwareComponent
    interfaces:
      cloudify.interfaces.lifecycle:
        start:
          implementation: fabric.fabric_plugin.tasks.run_task
          inputs:
            fabric_env:
              host_string: { get_attribute: [ master_host, public_ip ] }
              user: { get_input: ssh_user }
              key_filename: { get_input: ssh_keyfile }
            tasks_file: scripts/fabric_tasks.py
            task_name: start_master_bmc                
            task_properties:
              k8s_settings:
                k8s_version: v1.3.0
                etcd_version: 2.2.5
                flannel_version: v0.6.2
                flannel_ipmasq: 'true'
                flannel_network: 10.1.0.0/16
                flannel_backend: udp
                restart_policy: unless-stopped
                arch: amd64
                net_interface: eth0
    relationships:
      - type: cloudify.relationships.contained_in
        target: master_host

  worker:
    type: cloudify.nodes.SoftwareComponent
    interfaces:
      cloudify.interfaces.lifecycle:
        start:
          implementation: fabric.fabric_plugin.tasks.run_task
          inputs:
            fabric_env:
              host_string: { get_attribute: [ worker_host, public_ip ] }
              user: { get_input: ssh_user }
              key_filename: { get_input: ssh_keyfile }
            tasks_file: scripts/fabric_tasks.py
            task_name: start_worker_bmc
            task_properties:
              master_ip: { get_attribute: [ master_host, ip ] }
              k8s_settings:
                k8s_version: v1.3.0
                etcd_version: 2.2.5
                flannel_version: v0.6.2
                flannel_ipmasq: 'true'
                flannel_network: 10.1.0.0/16
                flannel_backend: udp
                restart_policy: unless-stopped
                arch: amd64
                net_interface: eth0
    relationships:
      - type: cloudify.relationships.depends_on
        target: master
      - type: cloudify.relationships.contained_in
        target: worker_host

  master_host:
    type: fabric_host
    properties:
      agent_config:
        install_method: remote
      bmc_config: *bmc_config
      ssh_keyfile: { get_input: master_key}
      name: master
      public_key_file: 
      image_id: { get_input: master_image }
      instance_shape: { get_input: master_shape }
      compartment_id: 
      availability_domain: { get_input: availability_domain }
    relationships:
      - type: cloudify.oraclebmc.relationships.instance_connected_to_subnet
        target: subnet

  worker_host:
    type: fabric_host
    properties:
      agent_config:
        install_method: remote
      bmc_config: *bmc_config
      ssh_keyfile: { get_input: worker_key}
      name: worker
      public_key_file: 
      image_id: { get_input: worker_image }
      instance_shape: { get_input: worker_shape }
      compartment_id: 
      availability_domain: { get_input: availability_domain }
    relationships:
      - type: cloudify.oraclebmc.relationships.instance_connected_to_subnet
        target: subnet
    interfaces:
      cloudify.interfaces.monitoring_agent:
        install:
          implementation: diamond.diamond_agent.tasks.install
          inputs:
            diamond_config:
              interval: 1
        start: diamond.diamond_agent.tasks.start
        stop: diamond.diamond_agent.tasks.stop
        uninstall: diamond.diamond_agent.tasks.uninstall
      cloudify.interfaces.monitoring:
        start:
          implementation: diamond.diamond_agent.tasks.add_collectors
          inputs:
            <<: *hyperkube_monitoring

  network:
    type: cloudify.oraclebmc.nodes.VCN
    properties:
      bmc_config: *bmc_config
      use_external_resource: true
      resource_id: 

  subnet:
    type: cloudify.oraclebmc.nodes.Subnet
    properties:
      bmc_config: *bmc_config
      name: kubernetes_subnet
      compartment_id: 
      cidr_block: 10.10.20.0/24
      availability_domain: 
      security_rules: 
        - "0.0.0.0/0,22"
        - "0.0.0.0/0,53"
        - "0.0.0.0/0,53,udp"
        - "0.0.0.0/0,443"
        - "0.0.0.0/0,8080"
        - "10.10.20.0/24,2379"
        - "10.10.20.0/24,4001"
        - "10.10.20.0/24,6443"
        - "10.10.20.0/24,8000"
        - "10.10.20.0/24,9090"
        - "10.10.20.0/24,10250"
    relationships:
      - type: cloudify.oraclebmc.relationships.subnet_in_network
        target: network

  gateway:
    type: cloudify.oraclebmc.nodes.Gateway
    properties:
      resource_id: 
      use_external_resource: true
      bmc_config: *bmc_config
    relationships:
      - type: cloudify.oraclebmc.relationships.gateway_connected_to_network
        target: network

groups:
 
  scale_up_group:
    members: [worker_host]
     # This defines a scale group whose members may be scaled up, incrementing by 1.
     # The scale worflow is called when the following criteria are met
     # The Hyperkube process total CPU will be more than 3 for a total of 10 seconds.
     # No more than 6 hosts will be allowed.
    policies:
      auto_scale_up:
        type: scale_policy_type
        properties:
          policy_operates_on_group: true
          scale_limit: 6
          scale_direction: '<'
          scale_threshold: 30
          service_selector: .*worker_host.*cpu.total.user
          cooldown_time: 60
        triggers:
          execute_scale_workflow:
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              workflow: scale
              workflow_parameters:
                delta: 1
                scalable_entity_name: worker
                scale_compute: true
 
  scale_down_group:
    # This defines a scale group whose members may be scaled down. Only one host will be removed per run.
    # The scale worflow is called when the following criteria are met
    # The Hyperkube process total CPU will be less than 1 for a total of 200 seconds.
    # No less than 2 hosts will be allowed.
    members: [worker_host]
    policies:
      auto_scale_down:
        type: scale_policy_type
        properties:
          scale_limit: 2
          scale_direction: '>'
          scale_threshold: 25
          #service_selector: .*worker_host.*.process.hyperkube.cpu.percent
          service_selector: .*worker_host.*cpu.total.user
          cooldown_time: 60
          moving_window_size: 30
        triggers:
          execute_scale_workflow:
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              workflow: scale
              workflow_parameters:
                delta: -1
                scalable_entity_name: worker
                scale_compute: true
 
  heal_group:
    # This defines a group of hosts in members that may be healed.
    # The heal workflow is called when a the following policy criteria are met.
    # Either the hyperkube process on the host, or the total host CPU need fall silent.
    # The host and all software that it is supposed to have running on it will be healed.
    members: [worker_host]
    policies:
      simple_autoheal_policy:
        type: cloudify.policies.types.host_failure
        properties:
          service:
            - .*worker_host.*.cpu.total.system
            - .*worker_host.*.process.hyperkube.cpu.percent
          interval_between_workflows: 60
        triggers:
          auto_heal_trigger:
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              workflow: heal
              workflow_parameters:
                node_instance_id: { 'get_property': [ SELF, node_id ] }
                diagnose_value: { 'get_property': [ SELF, diagnose ] }
 
outputs:
  kubernetes_info:
    description: Kubernetes Dashboard URL
    value:
      url: {concat: ["http://",{ get_attribute: [ master_host, public_ip ]},":8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard" ] }
