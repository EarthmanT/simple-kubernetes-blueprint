tosca_definitions_version: cloudify_dsl_1_3

imports:
  - http://www.getcloudify.org/spec/cloudify/3.4/types.yaml
  - http://www.getcloudify.org/spec/fabric-plugin/1.3.1/plugin.yaml
  - http://www.getcloudify.org/spec/diamond-plugin/1.3.3/plugin.yaml
  - https://github.com/cloudify-incubator/cloudify-oraclebmc-plugin/archive/0.1.zip
  - types/scale.yaml

dsl_definitions:
  bmc_config: &bmc_config
    user: 
    fingerprint: 
    key_file: ~/.ssh/bmcs_api_key
    tenancy: 
    region: us-phoenix-1

  hyperkube_monitoring: &hyperkube_monitoring
    collectors_config:
      CPUCollector: {}
      MemoryCollector: {}
      LoadAverageCollector: {}
      DiskUsageCollector:
        config:
          devices: sd[a-z]+[0-9]*$
      ProcessResourcesCollector:
        config:
          enabled: true
          unit: B
          measure_collector_time: true
          interval: 1
          process:
            hyperkube:
              name: hyperkube

inputs:
  ssh_user:
    default: opc
  ssh_keyfile:
    default: ''
  master_key:
    default: ''
  worker_key:
    default: ''
  master_image:
    description: image (must be Oracle Linux)
  master_shape:
    description: flavor
    default: BM.Standard1.36
  worker_image:
    description: image
  worker_shape:
    description: flavor
  availability_domain:
    description: availability domain

node_types:
  fabric_host:
    derived_from: cloudify.oraclebmc.nodes.Instance
    properties:
      ssh_keyfile:
        type: string
        default: { get_input: ssh_keyfile }

node_templates:

  master:
    type: cloudify.nodes.SoftwareComponent
    interfaces:
      cloudify.interfaces.lifecycle:
        start:
          implementation: fabric.fabric_plugin.tasks.run_commands
          inputs:
            fabric_env:
              host_string: { get_attribute: [ master_host, public_ip ] }
              user: { get_input: ssh_user }
              key_filename: { get_input: ssh_keyfile }
            commands:
              - "curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
              - "chmod +x kubectl"
              - "sudo setenforce 0"
              - "sudo systemctl disable firewalld"
              - "sudo systemctl stop firewalld"
              - "sudo yum install -y git"
              - "rm -rf kube-deploy"
              - "git clone https://github.com/kubernetes/kube-deploy"
              - "cd kube-deploy/docker-multinode;sudo ./master.sh"
    relationships:
      - type: cloudify.relationships.contained_in
        target: master_host

  worker:
    type: cloudify.nodes.SoftwareComponent
    interfaces:
      cloudify.interfaces.lifecycle:
        start:
          implementation: fabric.fabric_plugin.tasks.run_commands
          inputs:
            fabric_env:
              host_string: { get_attribute: [ worker_host, public_ip ] }
              user: { get_input: ssh_user }
              key_filename: { get_input: ssh_keyfile }
            commands:
              - "sudo yum install -y git"
              - "rm -rf kube-deploy"
              - "git clone https://github.com/kubernetes/kube-deploy"
              - { concat: [ "cd kube-deploy/docker-multinode;sudo MASTER_IP=", { get_attribute: [ master_host, private_ip ] }," ./worker.sh" ] }
    relationships:
      - type: cloudify.relationships.depends_on
        target: master
      - type: cloudify.relationships.contained_in
        target: worker_host

  master_host:
    type: fabric_host
    properties:
      agent_config:
        install_method: remote
      bmc_config: *bmc_config
      ssh_keyfile: { get_input: master_key}
      name: master
      public_key_file: ~/.ssh/dfilppi-dc.key.pub
      image_id: { get_input: master_image }
      instance_shape: { get_input: master_shape }
      compartment_id: ''
      availability_domain: { get_input: availability_domain }
    relationships:
      - type: cloudify.oraclebmc.relationships.instance_connected_to_subnet
        target: subnet

  worker_host:
    type: fabric_host
    properties:
      agent_config:
        install_method: remote
      bmc_config: *bmc_config
      ssh_keyfile: { get_input: worker_key}
      name: worker
      public_key_file: ~/.ssh/dfilppi-dc.key.pub
      image_id: { get_input: worker_image }
      instance_shape: { get_input: worker_shape }
      compartment_id: ''
      availability_domain: { get_input: availability_domain }
    relationships:
      - type: cloudify.oraclebmc.relationships.instance_connected_to_subnet
        target: subnet
    interfaces:
      cloudify.interfaces.monitoring_agent:
        install:
          implementation: diamond.diamond_agent.tasks.install
          inputs:
            diamond_config:
              interval: 1
        start: diamond.diamond_agent.tasks.start
        stop: diamond.diamond_agent.tasks.stop
        uninstall: diamond.diamond_agent.tasks.uninstall
      cloudify.interfaces.monitoring:
        start:
          implementation: diamond.diamond_agent.tasks.add_collectors
          inputs:
            <<: *hyperkube_monitoring

  network:
    type: cloudify.oraclebmc.nodes.VCN
    properties:
      bmc_config: *bmc_config
      use_external_resource: true
      resource_id: ''

  subnet:
    type: cloudify.oraclebmc.nodes.Subnet
    properties:
      bmc_config: *bmc_config
      name: kubernetes_subnet
      compartment_id: ''
      cidr_block: 10.10.20.0/24
      availability_domain: vNkz:PHX-AD-2
      security_rules: 
        - "0.0.0.0/0,22"
        - "0.0.0.0/0,53"
        - "0.0.0.0/0,53,udp"
        - "0.0.0.0/0,443"
        - "0.0.0.0/0,8080"
        - "10.10.20.0/24,2379"
        - "10.10.20.0/24,4001"
        - "10.10.20.0/24,6443"
        - "10.10.20.0/24,8000"
        - "10.10.20.0/24,9090"
        - "10.10.20.0/24,10250"
    relationships:
      - type: cloudify.oraclebmc.relationships.subnet_in_network
        target: network

  gateway:
    type: cloudify.oraclebmc.nodes.Gateway
    properties:
      resource_id: ''
      use_external_resource: true
      bmc_config: *bmc_config
    relationships:
      - type: cloudify.oraclebmc.relationships.gateway_connected_to_network
        target: network


groups:

 scale_up_group:
   members: [worker_host]
    # This defines a scale group whose members may be scaled up, incrementing by 1.
    # The scale worflow is called when the following criteria are met
    # The Hyperkube process total CPU will be more than 3 for a total of 10 seconds.
    # No more than 6 hosts will be allowed.
   policies:
     auto_scale_up:
       type: scale_policy_type
       properties:
         policy_operates_on_group: true
         scale_limit: 6
         scale_direction: '<'
         scale_threshold: 30
         #service_selector: .*worker_host.*.cpu.total.user
         service_selector: .*worker_host.*cpu.total.user
         cooldown_time: 60
       triggers:
         execute_scale_workflow:
           type: cloudify.policies.triggers.execute_workflow
           parameters:
             workflow: scale
             workflow_parameters:
               delta: 1
               scalable_entity_name: worker
               scale_compute: true

 scale_down_group:
   # This defines a scale group whose members may be scaled down. Only one host will be removed per run.
   # The scale worflow is called when the following criteria are met
   # The Hyperkube process total CPU will be less than 1 for a total of 200 seconds.
   # No less than 2 hosts will be allowed.
   members: [worker_host]
   policies:
     auto_scale_down:
       type: scale_policy_type
       properties:
         scale_limit: 2
         scale_direction: '>'
         scale_threshold: 25
         #service_selector: .*worker_host.*.process.hyperkube.cpu.percent
         service_selector: .*worker_host.*cpu.total.user
         cooldown_time: 60
         moving_window_size: 30
       triggers:
         execute_scale_workflow:
           type: cloudify.policies.triggers.execute_workflow
           parameters:
             workflow: scale
             workflow_parameters:
               delta: -1
               scalable_entity_name: worker
               scale_compute: true

 heal_group:
   # This defines a group of hosts in members that may be healed.
   # The heal workflow is called when a the following policy criteria are met.
   # Either the hyperkube process on the host, or the total host CPU need fall silent.
   # The host and all software that it is supposed to have running on it will be healed.
   members: [worker_host]
   policies:
     simple_autoheal_policy:
       type: cloudify.policies.types.host_failure
       properties:
         service:
           - .*worker_host.*.cpu.total.system
           - .*worker_host.*.process.hyperkube.cpu.percent
         interval_between_workflows: 60
       triggers:
         auto_heal_trigger:
           type: cloudify.policies.triggers.execute_workflow
           parameters:
             workflow: heal
             workflow_parameters:
               node_instance_id: { 'get_property': [ SELF, node_id ] }
               diagnose_value: { 'get_property': [ SELF, diagnose ] }

outputs:
  kubernetes_info:
    description: Kubernetes Dashboard URL
    value:
      url: {concat: ["http://",{ get_attribute: [ master_host, public_ip ]},":8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard" ] }
