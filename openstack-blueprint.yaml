###########################################################
# This Blueprint installs Kubernetes on Openstack
###########################################################

tosca_definitions_version: cloudify_dsl_1_3

description: >
  This blueprint creates a Kubernetes Cluster.
  It includes a master and two or more nodes with auto-scaling and auto-healing of the nodes.
  It is based on the Kubernetes Portable Multi-Node Cluster guide in the Kubernetes documentation website.
  https://kubernetes.io/docs/getting-started-guides/docker-multinode/

imports:
  - http://www.getcloudify.org/spec/cloudify/4.0/types.yaml
  - http://www.getcloudify.org/spec/openstack-plugin/2.0.1/plugin.yaml
  - http://www.getcloudify.org/spec/fabric-plugin/1.3.1/plugin.yaml
  - http://www.getcloudify.org/spec/diamond-plugin/1.3.5/plugin.yaml
  - types/scale.yaml
  - imports/kubernetes-blueprint.yaml

inputs:

  image:
    description: Image to be used when launching agent VM's

  flavor:
    description: Flavor of the agent VM's

  agent_user:
    description: >
      User for connecting to agent VM's
    default: ubuntu

  key_name:
    default: kubernetes-blueprint-key

  private_key_path:
    default: ~/.ssh/kubernetes-blueprint-key.pem

  external_network_name:
    default: external

  router_name:
    description: The Router Name

  public_network_name:
    description: The name of the Openstack public network.

  public_subnet_name:
    description: The name of the public network subnet.

  private_network_name:
    description: The name of the Openstack private network.

  private_subnet_name:
    description: The name of the private network subnet.

  region:
    default: ''

dsl_definitions:

  hyperkube_monitoring: &hyperkube_monitoring
    collectors_config:
      CPUCollector: {}
      MemoryCollector: {}
      LoadAverageCollector: {}
      DiskUsageCollector:
        config:
          devices: x?vd[a-z]+[0-9]*$
      NetworkCollector: {}
      ProcessResourcesCollector:
        config:
          enabled: true
          unit: B
          measure_collector_time: true
          cpu_interval: 0.5
          process:
            hyperkube:
              name: hyperkube

  openstack_config: &openstack_config
    username: { get_secret: keystone_username }
    password: { get_secret: keystone_password }
    tenant_name: { get_secret: keystone_tenant_name }
    auth_url: { get_secret: keystone_url }
    region: { get_input: region }

node_templates:

  kubernetes_master_host:
    type: cloudify.openstack.nodes.Server
    properties:
      openstack_config: *openstack_config
      agent_config:
        install_method: remote
        user: { get_input: agent_user }
        min_workers: 2
        key: { get_property: [ key, private_key_path ] }
      server:
        image: { get_input: image }
        flavor: { get_input: flavor }
        userdata: |
          #!/bin/bash
          sudo groupadd docker
          sudo gpasswd -a ubuntu docker
      management_network_name: { get_property: [ public_network, resource_id ] }
    relationships:
      - target: key
        type: cloudify.openstack.server_connected_to_keypair
      - target: kubernetes_master_port
        type: cloudify.openstack.server_connected_to_port

  kubernetes_master_port:
    type: cloudify.openstack.nodes.Port
    properties:
      openstack_config: *openstack_config
    relationships:
      - type: cloudify.relationships.contained_in
        target: public_network
      - type: cloudify.relationships.depends_on
        target: public_subnet
      - type: cloudify.openstack.port_connected_to_security_group
        target: kubernetes_security_group
      - type: cloudify.openstack.port_connected_to_floating_ip
        target: kubernetes_master_ip

  kubernetes_master_ip:
    type: cloudify.openstack.nodes.FloatingIP
    properties:
      openstack_config: *openstack_config
      floatingip:
        floating_network_name: { get_property: [ external_network, resource_id ] }

  kubernetes_security_group:
    type: cloudify.openstack.nodes.SecurityGroup
    properties:
      openstack_config: *openstack_config
      security_group:
        name: kubernetes_security_group
        description: kubernetes master security group
      rules:
      - remote_ip_prefix: 0.0.0.0/0
        port: 22
      - remote_ip_prefix: 0.0.0.0/0
        port: 53
      - remote_ip_prefix: 0.0.0.0/0
        port: 53
        protocol: udp
      - remote_ip_prefix: 0.0.0.0/0
        port: 80
      - remote_ip_prefix: 0.0.0.0/0
        port: 443
      - remote_ip_prefix: 0.0.0.0/0
        port: 2379
      - remote_ip_prefix: 0.0.0.0/0
        port: 4001
      - remote_ip_prefix: 0.0.0.0/0
        port: 6443
      - remote_ip_prefix: 0.0.0.0/0
        port: 8000
      - remote_ip_prefix: 0.0.0.0/0
        port: 8080
      - remote_ip_prefix: 0.0.0.0/0
        port: 9090
      - remote_ip_prefix: 0.0.0.0/0
        port: 10250

  kubernetes_node_host:
    # A virtual machine that will get a Kubernetes node installed on it.
    type: cloudify.openstack.nodes.Server
    properties:
      openstack_config: *openstack_config
      agent_config:
        install_method: remote
        user: { get_input: agent_user }
        min_workers: 2
        key: { get_property: [ key, private_key_path ] }
      server:
        image: {get_input: image}
        flavor: {get_input: flavor}
        userdata: |
          #!/bin/bash
          sudo groupadd docker
          sudo gpasswd -a ubuntu docker
      management_network_name: { get_property: [ private_network, resource_id ] }
    relationships:
      - type: cloudify.relationships.contained_in
        target: k8s_node_scaling_tier
      - target: kubernetes_node_port
        type: cloudify.openstack.server_connected_to_port
      - target: key
        type: cloudify.openstack.server_connected_to_keypair
    interfaces:
      cloudify.interfaces.monitoring_agent:
          install:
            implementation: diamond.diamond_agent.tasks.install
            inputs:
              diamond_config:
                interval: 1
          start: diamond.diamond_agent.tasks.start
          stop: diamond.diamond_agent.tasks.stop
          uninstall: diamond.diamond_agent.tasks.uninstall
      cloudify.interfaces.monitoring:
          start:
            implementation: diamond.diamond_agent.tasks.add_collectors
            inputs:
              <<: *hyperkube_monitoring

  kubernetes_node_port:
    type: cloudify.openstack.nodes.Port
    properties:
      openstack_config: *openstack_config
    relationships:
      - type: cloudify.relationships.contained_in
        target: k8s_node_scaling_tier
      - type: cloudify.relationships.connected_to
        target: private_network
      - type: cloudify.relationships.depends_on
        target: private_subnet
      - type: cloudify.openstack.port_connected_to_security_group
        target: kubernetes_security_group

  private_subnet:
    type: cloudify.openstack.nodes.Subnet
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: private_subnet_name }
    relationships:
      - target: private_network
        type: cloudify.relationships.contained_in

  private_network:
    type: cloudify.openstack.nodes.Network
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: private_network_name }

  public_subnet:
    type: cloudify.openstack.nodes.Subnet
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: public_subnet_name }
    relationships:
      - target: public_network
        type: cloudify.relationships.contained_in
      - target: router
        type: cloudify.openstack.subnet_connected_to_router

  public_network:
    type: cloudify.openstack.nodes.Network
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: public_network_name }

  router:
    type: cloudify.openstack.nodes.Router
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: router_name }
    relationships:
      - target: external_network
        type: cloudify.relationships.connected_to

  external_network:
    type: cloudify.openstack.nodes.Network
    properties:
      openstack_config: *openstack_config
      use_external_resource: true
      resource_id: { get_input: external_network_name }

  key:
    type: cloudify.openstack.nodes.KeyPair
    properties:
      openstack_config: *openstack_config
      resource_id: { get_input: key_name }
      private_key_path: { get_input: private_key_path }

  k8s_node_scaling_tier:
    type: cloudify.nodes.Root

groups:

  k8s_node_scale_group:
    members:
      - kubernetes_node_host
      - kubernetes_node_port

  scale_up_group:
    members: [kubernetes_node_host]
    # This defines a scale group whose members may be scaled up, incrementing by 1.
    # The scale worflow is called when the following criteria are met
    # The Hyperkube process total CPU will be more than 3 for a total of 10 seconds.
    # No more than 6 hosts will be allowed.
    policies:
      auto_scale_up:
        type: scale_policy_type
        properties:
          policy_operates_on_group: true
          scale_limit: 6
          scale_direction: '<'
          scale_threshold: 30
          #service_selector: .*kubernetes_node_host.*.cpu.total.user
          service_selector: .*kubernetes_node_host.*cpu.total.user
          cooldown_time: 60
        triggers:
          execute_scale_workflow:
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              workflow: scale
              workflow_parameters:
                delta: 1
                scalable_entity_name: kubernetes_node
                scale_compute: true

  scale_down_group:
    members: [kubernetes_node_host]
    # This defines a scale group whose members may be scaled up, incrementing by 1.
    # The scale worflow is called when the following criteria are met
    # The Hyperkube process total CPU will be more than 3 for a total of 10 seconds.
    # No more than 6 hosts will be allowed.
    policies:
      auto_scale_down:
        type: scale_policy_type
        properties:
          policy_operates_on_group: true
          scale_limit: 6
          scale_direction: '<'
          scale_threshold: 30
          #service_selector: .*kubernetes_node_host.*.cpu.total.user
          service_selector: .*kubernetes_node_host.*cpu.total.user
          cooldown_time: 60
        triggers:
          execute_scale_workflow:
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              workflow: scale
              workflow_parameters:
                delta: 1
                scalable_entity_name: kubernetes_node
                scale_compute: true
 
  heal_group:
  # This defines a group of hosts in members that may be healed.
  # The heal workflow is called when a the following policy criteria are met.
  # Either the hyperkube process on the host, or the total host CPU need fall silent.
  # The host and all software that it is supposed to have running on it will be healed.
    members: [kubernetes_node_host]
    policies:
      simple_autoheal_policy:
        type: cloudify.policies.types.host_failure
        properties:
          service:
            - .*kubernetes_node_host.*.cpu.total.system
            - .*kubernetes_node_host.*.process.hyperkube.cpu.percent
          interval_between_workflows: 60
        triggers:
          auto_heal_trigger:
            type: cloudify.policies.triggers.execute_workflow
            parameters:
              workflow: heal
              workflow_parameters:
                node_instance_id: { 'get_property': [ SELF, node_id ] }
                diagnose_value: { 'get_property': [ SELF, diagnose ] } 

policies:

  kubernetes_node_vms_scaling_policy:
    type: cloudify.policies.scaling
    properties:
      default_instances:  1
    targets: [k8s_node_scale_group]

outputs:
  kubernetes_info:
    description: Kubernetes Dashboard URL
    value:
      url: {concat: ["http://",{ get_attribute: [ kubernetes_master_ip, floating_ip_address ]},":8080/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard" ] }
